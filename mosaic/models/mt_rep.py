import copy
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from mosaic.models import get_model
from mosaic.models.discrete_logistic import DiscreteMixLogistic
from mosaic.models.rep_modules import BYOLModule, ContrastiveModule
from mosaic.models.basic_embedding import TemporalPositionalEncoding
from einops import rearrange, repeat, parse_shape
from collections import OrderedDict
import os
import hydra
from omegaconf import DictConfig, OmegaConf


class _StackedAttnLayers(nn.Module):
    """
    Returns all intermediate-layer outputs, in case we want to re-use features + add more losses later
                -> default sharing norms now!
    Demo and obs share the same batchnorm at every layer;
    Construct a stack of layers at the same time, note this would give us more control 
    over how the attended embedding of demonstration and observation is combined.
    attention matrics here are even _smaller_ if we set fuse_starts to >0
    Args to note
    - fuse_starts: counts for how many layers demo & obs should attend to itself, independent of each other 

    """

    def __init__(
        self,
        in_dim,
        out_dim,
        n_layers=3,
        fuse_starts=0,
        demo_ff_dim=128,
        obs_ff_dim=128,
        dropout=0,
        temperature=None,
        causal=False,
        n_heads=4,
        demo_T=4,
    ):
        super().__init__()
        assert demo_ff_dim % n_heads == 0, "n_heads must evenly divide feedforward_dim"
        self._n_heads = n_heads
        self._demo_ff_dim = demo_ff_dim
        self._obs_ff_dim = obs_ff_dim
        self._temperature = temperature if temperature is not None else np.sqrt(
            in_dim)

        self._obs_Qs, self._obs_Ks, self._obs_Vs = [
            nn.Sequential(*[nn.Conv3d(in_dim, obs_ff_dim, 1, bias=False)
                          for _ in range(n_layers)])
            for _ in range(3)]
        self._obs_Outs = nn.Sequential(
            *[nn.Conv3d(obs_ff_dim, out_dim, 1, bias=False) for _ in range(n_layers)])
        self._obs_a1s = nn.Sequential(
            *[nn.ReLU(inplace=dropout == 0) for _ in range(n_layers)])
        self._obs_drop1s = nn.Sequential(
            *[nn.Dropout3d(dropout) for _ in range(n_layers)])

        self._demo_Qs, self._demo_Ks, self._demo_Vs = [
            nn.Sequential(*[nn.Conv3d(in_dim, demo_ff_dim, 1, bias=False)
                          for _ in range(n_layers)])
            for _ in range(3)]
        self._demo_Outs = nn.Sequential(
            *[nn.Conv3d(demo_ff_dim, out_dim, 1, bias=False) for _ in range(n_layers)])
        self._demo_a1s = nn.Sequential(
            *[nn.ReLU(inplace=dropout == 0) for _ in range(n_layers)])
        self._demo_drop1s = nn.Sequential(
            *[nn.Dropout3d(dropout) for _ in range(n_layers)])
        self._norms = nn.Sequential(
            *[nn.BatchNorm3d(out_dim) for _ in range(n_layers)])

        self._n_layers = n_layers
        self._fuse_starts = fuse_starts
        print("A total of {} layers, cross demo-obs attention starts at layer idx {}".format(n_layers, fuse_starts))
        self._causal = causal  # keep causal only as an option for demo attention
        self._skip = out_dim == in_dim
        self._demo_T = demo_T

    def forward(self, inputs):
        """"""
        B, d, T, H, W = inputs.shape
        # obs_T could be as small as 1
        obs_T = T - self._demo_T
        out_dict = dict()
        for i in range(self._n_layers):
            demo_ly_in, obs_ly_in = inputs.split([self._demo_T, obs_T], dim=2)
            # -> (B, d, demo_T, H, W), (B, d, obs_T, H, W)
            # process demo first
            demo_q, demo_k, demo_v = [
                rearrange(conv[i](
                    demo_ly_in), 'B (head ch) T H W -> B head ch (T H W)', head=self._n_heads)
                for conv in [self._demo_Qs, self._demo_Ks, self._demo_Vs]
            ]
            # if self.query_task:
            #     demo_q = torch.cat((self.demo_q))
            a1, drop1 = [mod[i] for mod in [self._demo_a1s, self._demo_drop1s]]
            B, head, ch, THW = demo_q.shape
            ff_dim = head * ch
            # B, heads, T_demo*HW, T_demo*HW
            demo_kq = torch.einsum(
                'bnci,bncj->bnij', demo_k, demo_q) / self._temperature
            if self._causal:
                mask = torch.tril(torch.ones(
                    (self._demo_T, self._demo_T))).to(demo_kq.device)
                mask = mask.repeat_interleave(
                    H*W, 0).repeat_interleave(H*W, 1)  # -> (T*H*W, T*H*W)
                # -> (1, 1, T*H*W, T*H*W)
                demo_kq = demo_kq + torch.log(mask).unsqueeze(0).unsqueeze(0)
            demo_attn = F.softmax(demo_kq, 3)
            demo_v = torch.einsum('bncj,bnij->bnci', demo_v, demo_attn)
            demo_out = self._demo_Outs[i](
                rearrange(demo_v, 'B head ch (T H W) -> B (head ch) T H W',
                          T=self._demo_T, H=H, W=W)
            )
            demo_out = demo_out + \
                drop1(a1(demo_out)) if self._skip else drop1(demo_out)

            # now, repeat demo's K and V for obs. NOTE: **brought the T dimension forward**
            obs_q, obs_k, obs_v = [
                rearrange(conv[i](obs_ly_in),
                          'B (head ch) obs_T H W -> B obs_T head ch (H W)', head=self._n_heads)
                for conv in [self._obs_Qs, self._obs_Ks, self._obs_Vs]
            ]
            a1, drop1 = [mod[i] for mod in [self._obs_a1s, self._obs_drop1s]]
            if i >= self._fuse_starts:
                rep_k, rep_v = [
                    repeat(rep, 'B head ch THW -> B obs_T head ch THW', obs_T=obs_T) for rep in [demo_k, demo_v]]
                # only start attending to demonstration a few layers later
                # now cat_k is B, T, head, ch, (4+1)HW
                cat_k = torch.cat([rep_k, obs_k], dim=4)
                cat_v = torch.cat([rep_v, obs_v], dim=4)
            else:
                cat_k, cat_v = obs_k, obs_v  # only attend to observation selves
            obs_kq = torch.einsum(
                'btnci,btncj->btnij', cat_k, obs_q) / self._temperature  # B, obs_T, heads, (1+T_demo)*HW, 1*HW
            assert obs_kq.shape[-2] == (1+self._demo_T) * H * W or \
                obs_kq.shape[-2] == H*W
            # no causal mask is needed
            obs_attn = F.softmax(obs_kq, dim=4)
            obs_v = torch.einsum('btncj,btnji->btnci', cat_v, obs_attn)

            obs_out = self._obs_Outs[i](
                rearrange(
                    obs_v, 'B T heads ch (H W) -> B (heads ch) T H W', H=H, W=W, T=obs_T)
            )
            obs_ly_in = obs_ly_in + \
                drop1(a1(obs_out)) if self._skip else drop1(a1(obs_out))

            inputs = self._norms[i](torch.cat([demo_ly_in, obs_ly_in], dim=2))
            out_dict['out_%s' % i] = inputs
        out_dict['last'] = inputs
        return out_dict

    def freeze_layer(self, i):
        count = 0
        for conv in [self._demo_Qs, self._demo_Ks, self._demo_Vs] + [self._obs_Qs, self._obs_Ks, self._obs_Vs]:
            to_freeze = conv[i]
            for param in to_freeze.parameters():
                if param.requires_grad:
                    param.requires_grad = False
                    count += np.prod(param.shape)
        for mod in [self._demo_a1s, self._demo_drop1s, self._obs_a1s, self._obs_drop1s, self._norms]:
            to_freeze = mod[i]
            for param in to_freeze.parameters():
                if param.requires_grad:
                    param.requires_grad = False
                    count += np.prod(param.shape)
        return count


class _TransformerFeatures(nn.Module):
    """
    Transformer-like module for computing self-attention on convolution features
    """

    def __init__(
            self, latent_dim, demo_T=4, dim_H=7, dim_W=12, embed_hidden=256, dropout=0.2, n_attn_layers=2,
            pos_enc=True, attn_heads=4, attn_ff=128, just_conv=False,
            pretrained=True, img_cfg=None, drop_dim=2,
            causal=True, attend_demo=True, demo_out=True,
            fuse_starts=0):
        super().__init__()

        flag, drop_dim = img_cfg.network_flag, img_cfg.drop_dim
        self.network_flag = flag

        assert flag == 0, "flag number %s not supported!" % flag
        self._img_encoder = get_model('resnet')(
            output_raw=True, drop_dim=drop_dim, use_resnet18=True, pretrained=img_cfg.pretrained)
        if drop_dim == 2:
            conv_feature_dim = 512
        elif drop_dim == 3:
            conv_feature_dim = 256
        else:
            raise NotImplementedError

        self._attn_layers = _StackedAttnLayers(
            in_dim=conv_feature_dim, out_dim=conv_feature_dim, n_layers=n_attn_layers,
            demo_ff_dim=attn_ff, obs_ff_dim=attn_ff, dropout=dropout,
            causal=causal, n_heads=attn_heads, demo_T=demo_T, fuse_starts=fuse_starts,
        )

        self._pe = TemporalPositionalEncoding(
            conv_feature_dim, dropout) if pos_enc else None
        self.demo_out = demo_out
        in_dim = conv_feature_dim * dim_H * dim_W
        print("Linear embedder has input dim: {}x{}x{}={} ".format(
            conv_feature_dim, dim_H, dim_W, in_dim))

        self._linear_embed = nn.Sequential(
            nn.Linear(in_dim, embed_hidden),
            nn.Dropout(dropout), nn.ReLU(),
            nn.Linear(embed_hidden, latent_dim))

    def forward(self, images, context):
        assert len(
            images.shape) == 5, "expects [B, T, 3, height, width] tensor!"
        obs_T, demo_T = images.shape[1], context.shape[1]
        out_dict = OrderedDict()

        network_fn = self._resnet_features if self.network_flag == 0 else self._impala_features
        im_in = torch.cat((context, images), 1).float()
        im_features, no_pe_img_features = network_fn(im_in)
        out_dict['img_features'] = no_pe_img_features  # B T d H W
        out_dict['img_features_pe'] = rearrange(
            im_features, 'B d T H W -> B T d H W')
        # print(no_pe_img_features.shape)
        attn_out = self._attn_layers(im_features)
        # just use this for now, try other stuff later
        attn_features = attn_out['last']

        sizes = parse_shape(attn_features, 'B _ T _ _')
        features = rearrange(attn_features, 'B d T H W -> B T d H W', **sizes)
        out_dict['attn_features'] = features
        out_dict['demo_features'], out_dict['obs_features'] = \
            features.split([demo_T, obs_T], dim=1)
        # could also try do repre. on all intermediate layers too
        for k, v in attn_out.items():
            if k != 'last' and v.shape == attn_features.shape:
                reshaped = rearrange(v, 'B d T H W -> B T d H W', **sizes)
                out_dict['attn_'+k] = reshaped
                normalized = F.normalize(
                    self._linear_embed(rearrange(reshaped, 'B T d H W -> B T (d H W)')), dim=2)
                out_dict['attn_'+k+'_demo'], out_dict['attn_'+k +
                                                      '_img'] = normalized.split([demo_T, obs_T], dim=1)

        out_dict['linear_embed'] = self._linear_embed(
            rearrange(features, 'B T d H W -> B T (d H W)'))
        normalized = F.normalize(out_dict['linear_embed'], dim=2)
        out_dict['normed_linear_embed'] = normalized

        demo_embed, img_embed = normalized.split([demo_T, obs_T], dim=1)
        out_dict['demo_embed'] = demo_embed
        out_dict['demo_mean'] = torch.mean(demo_embed, dim=1)
        out_dict['img_embed'] = img_embed

        # NOTE(0427) this should always have length demo_T + obs_T now
        return out_dict

    def _resnet_features(self, x):
        if self._pe is None:
            return self._img_encoder(x)
        features = self._img_encoder(x)  # x is B, T, ch, h, w -> B, T, d, H, W
        pe_features = self._pe(features.transpose(1, 2))
        return pe_features, features  # B T d H W

    def _impala_features(self, x):
        # batch_size, concat_size = x.shape[0], x.shape[1] # B, T_c+T_im, 3, height, width
        sizes = parse_shape(x, 'B T _ _ _')
        x = rearrange(x, 'B T ch height width -> (B T) ch height width')
        features = self._img_encoder(x)  # B*T, d=256, H=6, W=9
        features = rearrange(features, '(B T) d H W -> B d T H W', **sizes)
        if self._pe is None:
            return features
        pe_features = self._pe(features)
        no_pe_features = rearrange(features, 'B d T H W -> B T d H W')

        return pe_features, no_pe_features


class _DiscreteLogHead(nn.Module):
    def __init__(self, in_dim, out_dim, n_mixtures, const_var=True, sep_var=False):
        super().__init__()
        assert n_mixtures >= 1, "must predict at least one mixture!"
        self._n_mixtures = n_mixtures
        self._dist_size = torch.Size((out_dim, n_mixtures))
        self._mu = nn.Linear(in_dim, out_dim * n_mixtures)
        self._logit_prob = nn.Linear(
            in_dim, out_dim * n_mixtures) if n_mixtures > 1 else None
        if const_var:
            ln_scale = torch.randn(
                out_dim, dtype=torch.float32) / np.sqrt(out_dim)
            self.register_parameter(
                '_ln_scale', nn.Parameter(ln_scale, requires_grad=True))
        if sep_var:
            ln_scale = torch.randn((out_dim, n_mixtures),
                                   dtype=torch.float32) / np.sqrt(out_dim)
            self.register_parameter(
                '_ln_scale', nn.Parameter(ln_scale, requires_grad=True))
        if not (const_var or sep_var):
            self._ln_scale = nn.Linear(in_dim, out_dim * n_mixtures)

    def forward(self, x):  #  x has shape B T d
        mu = self._mu(x).reshape((x.shape[:-1] + self._dist_size))

        if isinstance(self._ln_scale, nn.Linear):
            ln_scale = self._ln_scale(x).reshape(
                (x.shape[:-1] + self._dist_size))
        else:
            ln_scale = self._ln_scale if self.training else self._ln_scale.detach()
            if len(ln_scale.shape) == 1:
                ln_scale = ln_scale.reshape((1, 1, -1, 1)).expand_as(mu)
                # (1, 1, 8, 1) -> (B T, dist_size[0], dist_size[1]) i.e. each mixture has the **same** constant variance
            else:  # the sep_val case:
                ln_scale = repeat(
                    ln_scale, 'out_d n_mix -> B T out_d n_mix', B=x.shape[0], T=x.shape[1])

        logit_prob = self._logit_prob(x).reshape(
            mu.shape) if self._n_mixtures > 1 else torch.ones_like(mu)
        return (mu, ln_scale, logit_prob)


class VideoImitation(nn.Module):
    """ The imitation policy model  """

    def __init__(
        self,
        latent_dim,
        load_target_obj_detector=False,
        target_obj_detector_step=0,
        target_obj_detector_path=None,
        freeze_target_obj_detector=False,
        remove_class_layers=True,
        load_contrastive=True,
        concat_target_obj_embedding=True,
        height=120,
        width=160,
        demo_T=4,
        obs_T=6,
        dim_H=7,
        dim_W=12,
        action_cfg=None,
        attn_cfg=None,
        sdim=8,
        concat_state=False,
        atc_config=None,
        curl_config=None,
        concat_demo_head=False,
        concat_demo_act=False,
        demo_mean=0,
        byol_config=dict(),
        simclr_config=dict(),
    ):
        super().__init__()

        self._remove_class_layers = remove_class_layers
        if not load_target_obj_detector:
            self._embed = _TransformerFeatures(
                latent_dim=latent_dim, demo_T=demo_T, dim_H=dim_H, dim_W=dim_W, **attn_cfg)
        else:
            # load target object detector module
            conf_file = OmegaConf.load(os.path.join(
                target_obj_detector_path, "config.yaml"))
            self._embed, self._obj_classifier, self._target_obj_embedding = self._load_model(
                model_path=target_obj_detector_path, step=target_obj_detector_step, conf_file=conf_file, freeze=freeze_target_obj_detector, remove_class_layers=remove_class_layers)

        self._target_object_backbone = None
        if not freeze_target_obj_detector:
            self._target_object_backbone = copy.deepcopy(self._embed)
            for param in self._target_object_backbone.parameters():
                param.requires_grad = False

        if (not load_target_obj_detector or not freeze_target_obj_detector) and load_contrastive:
            self._target_embed = copy.deepcopy(self._embed)
            self._target_embed.load_state_dict(self._embed.state_dict())
            for p in self._target_embed.parameters():
                p.requires_grad = False

            # one auxillary module calculate multiple losses
            # create a dummy input to calculate feature dimensions here
            with torch.no_grad():
                x = torch.zeros((1, demo_T+obs_T, 3, height, width))

                _out = self._embed(images=x[:, :demo_T], context=x[:, demo_T:])
                img_feats = _out['img_features']
                print("Image feature dimensions: {}".format(img_feats.shape))
                _, _, img_conv_dim, _, _ = img_feats.shape
                attn_feats = _out['attn_features']
                _, _, attn_conv_dim, _, _ = attn_feats.shape
                # should both be B, demo_T+obs, _, _, _
                assert img_feats.shape[1] == attn_feats.shape[1] or img_feats.shape[1] == attn_feats.shape[1] + demo_T
                img_feat_dim = np.prod(img_feats.shape[2:])  # should be d*H*W
                attn_feat_dim = np.prod(attn_feats.shape[2:])
                if img_feat_dim != attn_feat_dim:
                    print("Warning! pre and post attn features have different shapes:",
                          img_feat_dim, attn_feat_dim)

            self._byol = BYOLModule(
                embedder=self._target_embed,
                img_feat_dim=img_feat_dim, attn_feat_dim=attn_feat_dim,
                img_conv_dim=img_conv_dim, attn_conv_dim=attn_conv_dim, **byol_config)
            self._simclr = ContrastiveModule(
                embedder=self._target_embed,
                img_feat_dim=img_feat_dim, attn_feat_dim=attn_feat_dim,
                img_conv_dim=img_conv_dim, attn_conv_dim=attn_conv_dim, **simclr_config)

        self._load_target_obj_detector = load_target_obj_detector
        self._freeze_target_obj_detector = freeze_target_obj_detector
        self._load_contrastive = load_contrastive
        self._demo_T = demo_T
        self._obs_T = obs_T
        self._concat_state = concat_state
        self._concat_target_obj_embedding = concat_target_obj_embedding
        # action processing
        assert action_cfg.n_mixtures >= 1, "must predict at least one mixture!"
        self.concat_demo_head = concat_demo_head
        self.concat_demo_act = concat_demo_act
        assert not (
            concat_demo_head and concat_demo_act), 'Only support one concat type'
        print("Concat-ing embedded demo to action head? {}, to distribution head? {}".format(
            concat_demo_act, concat_demo_head))

        # NOTE(Mandi): reduced input dimension size  from previous version! hence maybe try widen/add more action layers
        if concat_target_obj_embedding:
            target_obj_embedding_dim = self._target_obj_embedding._modules['2'].out_features
            ac_in_dim = int(latent_dim + float(concat_target_obj_embedding) * target_obj_embedding_dim +
                            float(concat_demo_act) * latent_dim + float(concat_state) * sdim)
        else:
            ac_in_dim = int(latent_dim + float(concat_demo_act) * latent_dim + float(concat_state) * sdim)

        inv_input_dim = int(2*ac_in_dim)

        if action_cfg.n_layers == 1:
            self._action_module = nn.Sequential(
                nn.Linear(ac_in_dim, action_cfg.out_dim), nn.ReLU())
            self._inv_model = nn.Sequential(
                nn.Linear(inv_input_dim, action_cfg.out_dim), nn.ReLU())
        elif action_cfg.n_layers == 2:
            self._action_module = nn.Sequential(
                nn.Linear(ac_in_dim, action_cfg.hidden_dim), nn.ReLU(),
                nn.Linear(action_cfg.hidden_dim, action_cfg.out_dim), nn.ReLU()
            )
            self._inv_model = nn.Sequential(
                nn.Linear(inv_input_dim, action_cfg.hidden_dim), nn.ReLU(),
                nn.Linear(action_cfg.hidden_dim, action_cfg.out_dim), nn.ReLU()
            )
        else:
            raise NotImplementedError

        head_in_dim = int(action_cfg.out_dim +
                          float(concat_demo_head) * latent_dim)
        self._action_dist = _DiscreteLogHead(
            in_dim=head_in_dim,
            out_dim=action_cfg.adim,
            n_mixtures=action_cfg.n_mixtures,
            const_var=action_cfg.const_var,
            sep_var=action_cfg.sep_var
        )
        self.demo_mean = demo_mean

        model_parameters = filter(lambda p: p.requires_grad, self.parameters())
        params = sum([np.prod(p.size()) for p in model_parameters])
        print('Total params in Imitation module:', params)

    def _load_model(self, model_path=None, step=0, conf_file=None, remove_class_layers=True, freeze=True):
        if model_path:
            # 1. Create the model starting from configuration
            model = hydra.utils.instantiate(conf_file.policy)
            # 2. Load weights
            weights = torch.load(os.path.join(
                model_path, f"model_save-{step}.pt"), map_location=torch.device('cpu'))
            model.load_state_dict(weights)
            # Remove classification layers
            if remove_class_layers:
                # Take the first _TransformerFeatures
                feature_extractor = list(model.children())[0]
                obj_classifiers = list(model.children())[-1]
                # do not take the classification layer
                target_obj_embedding_layers = list(
                    obj_classifiers.children())[:4]
                target_obj_embedding = nn.Sequential(
                    *target_obj_embedding_layers)
                for param in obj_classifiers.parameters():
                    param.requires_grad = False
                for param in target_obj_embedding.parameters():
                    param.requires_grad = False
            if freeze:
                for param in feature_extractor.parameters():
                    param.requires_grad = False

            return feature_extractor, obj_classifiers, target_obj_embedding
        else:
            raise ValueError("Model path cannot be None")

    def get_action(self, embed_out, target_obj_embedding=None, ret_dist=True, states=None, eval=False):
        """directly modifies output dict to put action outputs inside"""
        out = dict()
        # single-head case
        demo_embed, img_embed = embed_out['demo_embed'], embed_out['img_embed']
        assert demo_embed.shape[1] == self._demo_T

        if not eval:
            obs_T = self._obs_T  # img_embed.shape[1]
        else:
            obs_T = img_embed.shape[1]

        if self._concat_target_obj_embedding:
            ac_in = img_embed[:, 1:, :]
            states = states[:, 1:, :]
        elif self._concat_target_obj_embedding and eval:
            ac_in = img_embed
            states = states
        else:
            ac_in = img_embed

        if self.demo_mean:
            demo_embed = torch.mean(demo_embed, dim=1)
        else:
            # only take the last image, should alread be attended tho
            demo_embed = demo_embed[:, -1, :]
        demo_embed = repeat(demo_embed, 'B d -> B ob_T d', ob_T=obs_T)

        if self._concat_target_obj_embedding and not eval:
            target_obj_embedding = target_obj_embedding.repeat(1, obs_T, 1)
            img_embed = img_embed[:, 1:, :]
            img_embed = torch.cat((img_embed, target_obj_embedding), dim=2)
        elif self._concat_target_obj_embedding and eval:
            target_obj_embedding = target_obj_embedding.repeat(1, obs_T, 1)
            img_embed = img_embed
            img_embed = torch.cat((img_embed, target_obj_embedding), dim=2)

        if self.concat_demo_act:  # for action model
            ac_in = torch.cat((img_embed, demo_embed), dim=2)
            ac_in = F.normalize(ac_in, dim=2)
        ac_in = torch.cat((ac_in, states), 2) if self._concat_state else ac_in

        # predict behavior cloning distribution
        ac_pred = self._action_module(ac_in)
        if self.concat_demo_head:
            ac_pred = torch.cat((ac_pred, demo_embed), dim=2)
            # maybe better to normalize here
            ac_pred = F.normalize(ac_pred, dim=2)

        mu_bc, scale_bc, logit_bc = self._action_dist(ac_pred)
        out['bc_distrib'] = DiscreteMixLogistic(mu_bc, scale_bc, logit_bc) \
            if ret_dist else (mu_bc, scale_bc, logit_bc)
        out['demo_embed'] = demo_embed
        out['img_embed'] = img_embed
        # multi-head case? maybe register a name for each action head
        return out

    def _compute_target_obj_embedding(self, embed_out):
        # 1. Take embedding computed from the demo and the agent
        # Take only the first frame for the agent scene
        demo_embed, img_embed = embed_out['demo_embed'], embed_out['img_embed'][:, 0, :][:, None, :]

        assert demo_embed.shape[1] == self._demo_T
        obs_T = img_embed.shape[1]
        if self.demo_mean:
            demo_embed = torch.mean(demo_embed, dim=1)
        else:
            # only take the last image, should alread be attended tho
            demo_embed = demo_embed[:, -1, :]
        demo_embed = repeat(demo_embed, 'B d -> B ob_T d', ob_T=obs_T)
        ac_in = torch.cat((img_embed, demo_embed), dim=2)
        ac_in = F.normalize(ac_in, dim=2)

        target_object_embedding = self._target_obj_embedding(ac_in)

        return target_object_embedding

    def forward(
        self,
        images,
        context,
        states=None,
        ret_dist=True,
        eval=False,
        images_cp=None,
        context_cp=None,
        actions=None,
        target_obj_embedding=None
    ):
        B, obs_T, _, height, width = images.shape
        demo_T = context.shape[1]
        if not eval:
            assert images_cp is not None, 'Must pass in augmented version of images'
        embed_out = self._embed(images, context)

        if self._target_object_backbone is not None and not self._freeze_target_obj_detector:
            # compute the embedding for the first frame
            first_frame_batch = images[:, 0, :, :, :]
            first_frame_batch = first_frame_batch[:, None, :, :, :]
            target_obj_embedding_in = self._target_object_backbone(
                first_frame_batch, context)
        else:
            target_obj_embedding_in = embed_out

        if self._concat_target_obj_embedding and target_obj_embedding is None:
            # compute the target obj embedding, given the input from the first frame
            target_obj_embedding = self._compute_target_obj_embedding(
                target_obj_embedding_in)

        out = self.get_action(
            embed_out=embed_out, target_obj_embedding=target_obj_embedding, ret_dist=ret_dist, states=states, eval=eval)

        if self._concat_target_obj_embedding:
            out["target_obj_embedding"] = target_obj_embedding

        if eval:
            return out  # NOTE: early return here to do less computation during test time

        if (not self._load_target_obj_detector or not self._freeze_target_obj_detector) and self._load_contrastive:
            # run frozen transformer on augmented images
            embed_out_target = self._target_embed(images_cp, context_cp)

            byol_out_dict = self._byol(embed_out, embed_out_target)
            for k, v in byol_out_dict.items():
                assert 'byol' in k
                out[k] = v

            simclr_out_dict = self._simclr(embed_out, embed_out_target)
            for k, v in simclr_out_dict.items():
                assert 'simclr' in k
                out[k] = v

        # run inverse model
        if self._concat_target_obj_embedding:
            # both are (B ob_T d)
            demo_embed, img_embed = out['demo_embed'], embed_out['img_embed'][:, 1:, :]
            states = states[:, 1:, :]
        else:
            demo_embed, img_embed = out['demo_embed'], embed_out['img_embed']

        # B, T_im-1, d * 2
        inv_in = torch.cat((img_embed[:, :-1], img_embed[:, 1:]), 2)
        if self.concat_demo_act:
            inv_in = torch.cat(
                (
                    F.normalize(
                        torch.cat((img_embed[:, :-1], demo_embed[:, :-1]), dim=2), dim=2),
                    F.normalize(
                        torch.cat((img_embed[:,  1:], demo_embed[:, :-1]), dim=2), dim=2),
                ),
                dim=2)
            if self._concat_target_obj_embedding:
                target_obj_embedding_inv = target_obj_embedding.repeat(
                    1, inv_in.size(dim=1), 1)
                inv_in = torch.cat(
                    (
                        F.normalize(torch.cat(
                            (img_embed[:, :-1], demo_embed[:, :-1], target_obj_embedding_inv), dim=2), dim=2),
                        F.normalize(torch.cat(
                            (img_embed[:,  1:], demo_embed[:, :-1], target_obj_embedding_inv), dim=2), dim=2),
                    ),
                    dim=2)

            # print(inv_in.shape)
        if self._concat_state:
            inv_in = torch.cat(
                (torch.cat((inv_in, states[:, :-1]), dim=2), states[:, 1:]), dim=2)

        inv_pred = self._inv_model(inv_in)
        if self.concat_demo_head:
            inv_pred = torch.cat((inv_pred, demo_embed[:, :-1]), dim=2)
            # maybe better to normalize here
            inv_pred = F.normalize(inv_pred, dim=2)

        mu_inv, scale_inv, logit_inv = self._action_dist(inv_pred)
        out['inverse_distrib'] = DiscreteMixLogistic(mu_inv, scale_inv, logit_inv) \
            if ret_dist else (mu_inv, scale_inv, logit_inv)

        return out

    def momentum_update(self, frac):
        self._byol.update_mom(frac)
        return

    def soft_param_update(self):
        self._byol.soft_param_update()
        tau = 1 - self._byol.mom
        for param, target_param in zip(self._embed.parameters(), self._target_embed.parameters()):
            target_param.data.copy_(
                tau * param.data + (1 - tau) * target_param.data)
        return

    def freeze_attn_layers(self, n_layers=2):
        assert n_layers <= self._embed._attn_layers._n_layers, 'Attention only has %s layers' % self._embed._attn_layers._n_layers
        count = 0
        for n in range(n_layers):
            num_frozen = self._embed._attn_layers.freeze_layer(n)
            count += num_frozen
        print("Warning! Freeze the _First_ {} layers of attention! A total of {} params are frozen \n".format(
            n_layers, count))

    def freeze_img_encoder(self):
        count = 0
        for p in self._embed._img_encoder.parameters():
            if p.requires_grad:
                p.requires_grad = False
                count += np.prod(p.shape)
        print("Warning! Freeze %s parameters in the image encoder \n" % count)

    def restart_action_layers(self):
        count = 0
        for module in [self._action_module, self._action_dist, self._inv_model]:
            for layer in module.children():
                if hasattr(layer, 'reset_parameters'):
                    layer.reset_parameters()
                for p in layer.parameters():
                    count += np.prod(p.shape) if p.requires_grad else 0
        print("Re-intialized a total of %s parameters in action MLP layers" % count)

    def skip_for_eval(self):
        """ skip module inferences during evaluation"""
        self._byol = None
        self._simclr = None
        self._inv_model = None
        self._target_embed = None

    # def pretrain_img_encoder(self):
    #     """Freeze everything except for the image encoder + attention layers"""
    #     count = 0
    #     for p in zip(
    #         self._action_module.parameters(),
    #         self._inv_model.parameters(),
    #         self._embed._linear_embed.parameters()
    #         ):
    #         p.requires_grad = False
    #         count += np.prod(p.shape)
    #     print("Freezing action, inv, linear embed {} layer parameters".format(count))
